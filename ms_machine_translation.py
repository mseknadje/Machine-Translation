# -*- coding: utf-8 -*-
"""MS_Machine Translation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dLbimYR6zTn1KCMfGOrhr5IDoP8O2Ep3

# üìù üåç Machine Translation Project 
Project goal: build a neural machine translation model using a Transformer encoder-decoder architecture. This project implements the transformer from scratch, without using existing library implementations of Transformers, building the pieces that go into the full architecture.

Main components for this project include:

1. Implementing a Transformer encoder block and a Transformer decoder block
2. Implementing the BLEU metric for evaluating machine translation
3. Visualizing the decoder attention during translation

## Setup and Data Preprocessing
"""

import random
import math
import copy
import io
import os

import torch
import torch.nn as nn
import numpy as np
from collections import Counter

import torchtext
from torchtext.data.utils import get_tokenizer
from torchtext.utils import download_from_url, extract_archive
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader
from timeit import default_timer as timer

SEED = 42
def seed_everything(s):
  random.seed(s)
  torch.manual_seed(s)
  torch.cuda.manual_seed_all(s)
  np.random.seed(s)
  torch.backends.cudnn.deterministic = True
seed_everything(SEED)

!python -m spacy download en
!python -m spacy download de

# data loaded from multi30k dataset
url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'
train_urls = ('train.de.gz', 'train.en.gz')
val_urls = ('val.de.gz', 'val.en.gz')
test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')

train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]
val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]
test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]

de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')
en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')

def build_vocab(filepath, tokenizer):
  counter = Counter()
  with io.open(filepath, encoding="utf8") as f:
    for string_ in f:
      counter.update(tokenizer(string_))
  return torchtext.vocab.vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])

de_vocab = build_vocab(train_filepaths[0], de_tokenizer)
en_vocab = build_vocab(train_filepaths[1], en_tokenizer)
de_vocab.set_default_index(0)
en_vocab.set_default_index(0)

def data_process(filepaths):
  raw_de_iter = iter(io.open(filepaths[0], encoding="utf8"))
  raw_en_iter = iter(io.open(filepaths[1], encoding="utf8"))
  data = []
  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):
    de_tensor_ = torch.tensor(de_vocab(de_tokenizer(raw_de)), dtype=torch.long)
    en_tensor_ = torch.tensor(en_vocab(en_tokenizer(raw_en)), dtype=torch.long)

    data.append((de_tensor_, en_tensor_))
  return data

train_data = data_process(train_filepaths)
val_data = data_process(val_filepaths)
test_data = data_process(test_filepaths)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
BATCH_SIZE = 32
PAD_IDX = de_vocab['<pad>']
BOS_IDX = de_vocab['<bos>']
EOS_IDX = de_vocab['<eos>']

def generate_batch(data_batch):
  # data_batch is a list of tuples of (de_sentence, en_sentence)
  de_batch, en_batch = [], []
  for (de_item, en_item) in data_batch:
    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))
    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))
  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)
  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)
  return de_batch, en_batch

train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,
                        shuffle=True, collate_fn=generate_batch)
val_loader = DataLoader(val_data, batch_size=BATCH_SIZE,
                        shuffle=False, collate_fn=generate_batch)
test_loader = DataLoader(test_data, batch_size=BATCH_SIZE,
                       shuffle=False, collate_fn=generate_batch)

"""# Part 1: Building a Transformer
Implements our own Transformer model using some PyTorch implementations of attention. Transformers were introduced in 2017 by Vaswani et al. and consist of an encoder-decoder structure.

First, we'll implement the green encoder box, and then, we'll implement the pink decoder. We'll then link these modules together into a `ManualTransformer`.

Modules:
* `TokenEmbedding`, basically a PyTorch `nn.Embedding` module except it also multiplies the embeddings by $\sqrt{d_{model}}$ as specified in Vaswani et al. 2017
* `PositionalEncoding`, a PyTorch module whose forward pass adds positional encodings to the inputted embeddings
* **\*\*** **`SublayerConnection`**, a module that takes in a sublayer function (e.g. the forward pass of a feedforward layer or the forward pass of an attention module) and performs layer normalization and dropout on that sublayer
* **\*\*** **`PositionwiseFeedForward`**, a module that implements the feedforward network used throughout the Transformer

Helper functions:
* `generate_subsequent_mask(size)` which generates a mask to obscure future positions for the decoder
* `padding_mask(idx_tensor)` which takes in a tensor of word indices and outputs a mask that is `True` wherever there is padding in the given tensor
* **\*\*** **`clones(module, N)`** which takes in a module and returns a `nn.ModuleList` of N clones of that module.
"""

class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size, emb_size):
        super(TokenEmbedding, self).__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.emb_size = emb_size

    def forward(self, tokens):
        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)
  
class PositionalEncoding(nn.Module):
    def __init__(self, emb_size, dropout, maxlen = 5000):
        super(PositionalEncoding, self).__init__()
        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
        pos_embedding = torch.zeros((maxlen, emb_size))
        pos_embedding[:, 0::2] = torch.sin(pos * den)
        pos_embedding[:, 1::2] = torch.cos(pos * den)
        pos_embedding = pos_embedding.unsqueeze(-2)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer('pos_embedding', pos_embedding)

    def forward(self, token_embedding):
        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])

class SublayerConnection(nn.Module):
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = nn.LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))

class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(self.w_1(x).relu()))

def generate_subsequent_mask(size):
    mask = (torch.triu(torch.ones((size, size), device=device)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask

def padding_mask(idx_tensor):
  return (idx_tensor == PAD_IDX).transpose(0, 1)

def clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

def train_epoch(model, optimizer, train_dataloader, loss_fn):
    model.train()
    total_loss = 0

    for src, tgt in train_dataloader:
        src, tgt = src.to(device), tgt.to(device)
        tgt_input = tgt[:-1, :]

        src_padding_mask = padding_mask(src)
        tgt_padding_mask = padding_mask(tgt_input)

        logits = model(src, tgt_input, src_padding_mask, tgt_padding_mask)

        optimizer.zero_grad()

        tgt_out = tgt[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
        loss.backward()

        optimizer.step()
        total_loss += loss.item()

    return total_loss / len(train_dataloader)


def val(model, val_dataloader, loss_fn):
    model.eval()
    losses = 0

    for src, tgt in val_dataloader:
        src = src.to(device)
        tgt = tgt.to(device)

        tgt_input = tgt[:-1, :]

        src_padding_mask = padding_mask(src)
        tgt_padding_mask = padding_mask(tgt_input)

        logits = model(src, tgt_input, src_padding_mask, tgt_padding_mask)

        tgt_out = tgt[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
        losses += loss.item()

    return losses / len(val_dataloader)

def train(model, loss_fn, optimizer, n_epochs=10):
  model = model.to(device)

  for epoch in range(1, n_epochs+1):
      start_time = timer()
      train_loss = train_epoch(model, optimizer, train_loader, loss_fn)
      end_time = timer()
      val_loss = val(model, val_loader, loss_fn)
      print((f"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, "f"Epoch time = {(end_time - start_time):.3f}s"))

"""## Part 1a: Building an Encoder Block

One green Transformer encoder box consists of *N* identical blocks stacked on top of each other. 

To implement one of these green layers, we create a `ManualEncoderLayer` class, and then duplicate and stack these all together in our `ManualEncoder` class.

Each encoder layer consists of:
1. A self-attention sublayer (hint: `torch.nn.MultiheadAttention`)
2. A feed-forward sublayer (hint: the class we give you, `PositionWiseFeedForward`)
"""

class ManualEncoderLayer(nn.Module):
    def __init__(self, dim_emb, dropout, nhead, dim_ff):
        super(ManualEncoderLayer, self).__init__()
        # Initialize the necessary pieces of the encoder block
        # create self-attention sublayer
        # create feed-forward sublayer 
        # create a new SublayerConnection module for each sublayer
        self.dim_emb = dim_emb
        self.dropout = dropout
        self.nhead = nhead
        self.dim_ff = dim_ff
        self.multihead_attn = nn.MultiheadAttention(dim_emb, nhead, dropout=dropout)
        self.self_attn_sub = SublayerConnection(dim_emb, dropout)
        self.feed_forward = PositionwiseFeedForward(dim_emb, dim_ff, dropout=dropout)
        self.feed_forw_sub = SublayerConnection(dim_ff, dropout)
      

    def forward(self, x, src_mask, padding_mask):
      # implement residual connections and a LayerNorm module (normalizing)
      def self_attention(x):
        # pass arguments using mask=
        attn_output, attn_output_weights = self.multihead_attn(x, x, x, key_padding_mask=padding_mask, attn_mask=src_mask)
        return attn_output
      
      # pass function calling attention module or feed-forward module
      attention = self.self_attn_sub.forward(x, self_attention)
      ff = self.feed_forw_sub.forward(attention, self.feed_forward)
      return ff

"""In the encoder itself, we
1. create *N* instances of the given `ManualEncoderLayer` 
2. stack together your *N* layers in the forward pass
3. perform layer normalization (`nn.LayerNorm`) one more time at the end
"""

class ManualEncoder(nn.Module):
    def __init__(self, layer, N):
      # Initialize the necessary pieces of the encoder 
        super(ManualEncoder, self).__init__()
        self.layer = layer
        self.N = N
        self.layers = clones(self.layer, self.N)
        self.layer_norm = nn.LayerNorm(self.layer.dim_emb)

    def forward(self, x, src_mask, padding_mask):
      # loop through the all the layers N times
      # feed the output into the next layer
      # take final output and normalize it

      for layer in self.layers:
        output = layer.forward(x, src_mask, padding_mask)
        x = output
      
      output = self.layer_norm(x)
      return output

"""### Testing the Encoder
```
tensor([[ 0.7457,  0.8852,  1.0867, -1.8108,  0.8940, -0.2294, -0.5474, -1.0240],
        [-0.5925,  1.6613,  0.9158, -0.4786, -1.1084,  1.1541, -0.9447, -0.6070],
        [ 1.0442,  0.8725,  1.2680, -1.8293,  0.2005, -0.6618, -0.8446, -0.0495],
        [ 0.8902,  1.7593, -0.0814, -1.5282,  0.2167,  0.0962, -1.3084, -0.0443],
        [-0.1739,  2.1798,  0.0213, -0.8473, -0.4256,  0.4360, -1.4378,  0.2474]],
       grad_fn=<NativeLayerNormBackward0>)
```
"""

seed_everything(SEED)
test_encoder = ManualEncoder(ManualEncoderLayer(dim_emb=8, dropout=0.1, nhead=8, dim_ff=8), 2)
encoder_tester_srcmask = torch.zeros((5, 5)).type(torch.bool)
encoder_tester_paddingmask = torch.cat([torch.zeros((3,)), torch.ones((2,))]).type(torch.bool) # pretend the last two tokens are <pad>
encoder_tester = torch.Tensor([[0.9806, 0.0063, 0.6183, 0.1056, 0.9913, 0.5688, 0.6567, 0.1243],
        [0.5095, 0.3469, 0.7348, 0.8514, 0.4808, 0.7631, 0.6010, 0.1722],
        [0.9106, 0.3521, 0.3979, 0.0212, 0.1941, 0.0814, 0.2538, 0.0634],
        [0.7813, 0.0269, 0.1815, 0.3024, 0.1320, 0.6719, 0.6892, 0.4477],
        [0.4092, 0.4568, 0.5662, 0.5217, 0.0281, 0.4834, 0.3580, 0.5059]]) # (source sequence length=5, emb_size=8)
test_encoder.eval()
test_encoder_out = test_encoder(encoder_tester, encoder_tester_srcmask, encoder_tester_paddingmask)
print(test_encoder_out)



"""## Part 1b: Building a Decoder Block

A single decoder layer consists of:
1. A self-attention sublayer
2. A cross-attention sublayer (encoder-decoder attention)
3. A feed-forward sublayer
"""

class ManualDecoderLayer(nn.Module):
    def __init__(self, dim_emb, dropout, nhead, dim_ff):
        super(ManualDecoderLayer, self).__init__()
        
        #Initialize the necessary pieces of the decoder block
        self.attn_weights = None
        self.dim_emb = dim_emb
        self.dropout = dropout
        self.nhead = nhead
        self.dim_ff = dim_ff
        self.multihead_attn = nn.MultiheadAttention(self.dim_emb, self.nhead, dropout=self.dropout)
        self.encoder_decoder_layer = nn.MultiheadAttention(self.dim_emb, self.nhead, dropout=self.dropout)
        self.feed_forward = PositionwiseFeedForward(d_model=self.dim_emb, d_ff=self.dim_ff, dropout=self.dropout)
        self.feed_forw_sub = SublayerConnection(dim_ff, dropout)
        self.sub_layer = SublayerConnection(size=self.dim_emb, dropout=self.dropout)
        self.sub_layer2 = SublayerConnection(size=self.dim_emb, dropout=self.dropout)
        self.sub_layer3 = SublayerConnection(size=self.dim_emb, dropout=self.dropout)  


    def forward(self, x, memory, src_padding_mask, tgt_mask, tgt_padding_mask):
      def self_attention(x):
        attn_output, attn_out_weights = self.multihead_attn(x, x, x, key_padding_mask=tgt_padding_mask, attn_mask=tgt_mask)
        return attn_output

      # save the cross-attention weights
      def cross_attention(x):
        # use memory argument as the encoder outputs, passing it to cross-attention sublayer
        en_de_output, en_de_output_weights = self.encoder_decoder_layer(x, memory, memory, key_padding_mask=src_padding_mask)
        return en_de_output

      def ff_function(x):
        ff_out = self.feed_forward.forward(x)
        return ff_out

      # implement the forward pass
      self_att_outputs = self.sub_layer.forward(x, self_attention)
      encoder_decoder_outputs = self.sub_layer2.forward(self_att_outputs, cross_attention)
      ff_outputs = self.sub_layer3.forward(encoder_decoder_outputs, ff_function)

      cross_attn, cross_attn_weights = self.multihead_attn(encoder_decoder_outputs, memory, memory, need_weights=True)
      self.attn_weights = cross_attn_weights
      return ff_outputs

"""We can combine as many decoder layers as we want in our decoder class, which should look similar to the `ManualEncoder` class.

1. create *N* instances of the given `ManualDecoderLayer` 
2. stack together your *N* layers in the forward pass
3. perform layer normalization (`nn.LayerNorm`) one more time at the end
"""

class ManualDecoder(nn.Module):
    def __init__(self, layer, N):
        super(ManualDecoder, self).__init__()
        # Initialize the necessary pieces of the decoder 
        self.layer = layer
        self.N = N
        self.layers = clones(self.layer, self.N)
        self.layer_norm = nn.LayerNorm(layer.dim_emb)

    def forward(self, x, memory, src_padding_mask, tgt_mask, tgt_padding_mask):
      for layer in self.layers:
        x = layer.forward(x, memory, src_padding_mask, tgt_mask, tgt_padding_mask)

      output = self.layer_norm(x)
      return output

"""### Testing the Decoder
```
tensor([[ 1.2406, -0.9287,  1.3650, -1.3399,  0.1385, -0.9783,  0.8983, -0.3955],
        [ 0.9685,  0.0181,  1.3477, -2.1036,  0.1635, -0.3701,  0.5607, -0.5848],
        [ 0.6339,  1.2617, -1.0904, -1.2991,  0.8132,  0.0155,  0.9308, -1.2656],
        [ 1.3019, -0.2461,  0.5263, -0.8281,  0.1514, -1.2825,  1.5194, -1.1424],
        [ 0.7919, -0.3827,  0.8741, -0.8170,  0.7603, -0.9159,  1.3159, -1.6267],
        [ 1.4759,  0.0240,  0.0938, -1.4089,  0.2116, -0.7633,  1.4349, -1.0681]],
       grad_fn=<NativeLayerNormBackward0>)
```


"""

seed_everything(SEED)
test_decoder = ManualDecoder(ManualDecoderLayer(dim_emb=8, dropout=0.1, nhead=8, dim_ff=8), 2)
decoder_tester_srcmask = torch.zeros((5,)).type(torch.bool)
decoder_tester_tgtmask = generate_subsequent_mask(6).to("cpu")
decoder_tester_tgtpaddingmask = torch.cat([torch.zeros((5,)), torch.ones((1,))]).type(torch.bool) # pretend the last token is <pad>
decoder_tester = torch.Tensor([[0.5398, 0.2187, 0.2749, 0.6701, 0.0616, 0.4221, 0.4889, 0.2974],
        [0.6820, 0.5322, 0.8510, 0.4556, 0.1844, 0.8533, 0.5698, 0.3568],
        [0.1704, 0.6172, 0.0231, 0.6317, 0.7472, 0.0614, 0.1727, 0.1405],
        [0.9203, 0.9251, 0.5313, 0.9694, 0.3328, 0.0146, 0.6746, 0.6771],
        [0.9902, 0.4999, 0.8832, 0.8065, 0.8619, 0.0038, 0.9117, 0.0738],
        [0.9483, 0.4793, 0.0569, 0.6390, 0.3982, 0.0509, 0.3187, 0.5516]]) # (source sequence length=6, emb_size=8)
m = torch.Tensor([[ 5.1795e-02, -3.3640e-01, -7.0393e-01, -7.7838e-01,  2.3397e+00, -2.0453e-03, -1.0419e+00,  4.7115e-01],
        [-1.1013e+00,  1.6081e+00, -1.1974e+00,  1.2322e+00, -6.1193e-01, 7.6468e-01, -5.0261e-01, -1.9163e-01],
        [ 8.5518e-01,  1.1917e+00, -4.1526e-01, -1.5853e+00, -1.8449e-01, -9.6848e-01, -3.3488e-01,  1.4416e+00],
        [-1.3389e-01,  1.1363e+00, -2.2511e+00,  3.1282e-02, -2.9551e-01, 1.4653e-01,  1.4476e-01,  1.2216e+00],
        [-8.5308e-01,  1.8265e+00, -1.3266e+00,  3.6617e-01, -6.4322e-01, 1.4753e-01, -6.2170e-01,  1.1044e+00]]) # test_encoder_out 

test_decoder.eval()
print(test_decoder(decoder_tester, m, decoder_tester_srcmask, decoder_tester_tgtmask, decoder_tester_tgtpaddingmask))

"""## Part 1c: Putting Everything Together

Finally, we can implement the last few details. We've already taken care of mostly everything on here (recognize the multi-head attention, layer normalization, and feed-forward networks).
"""

class ManualTransformer(nn.Module):
    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead, 
                 src_vocab_size, tgt_vocab_size, dim_feedforward = 512, dropout = 0.1):
        super(ManualTransformer, self).__init__()
        self.encoder = ManualEncoder(ManualEncoderLayer(emb_size, dropout, nhead, dim_feedforward), num_encoder_layers)
        self.decoder = ManualDecoder(ManualDecoderLayer(emb_size, dropout, nhead, dim_feedforward), num_decoder_layers)
        self.src_embed = TokenEmbedding(src_vocab_size, emb_size)
        self.tgt_embed = TokenEmbedding(tgt_vocab_size, emb_size)
        self.positional_encoding = PositionalEncoding(emb_size, dropout)
        self.out = nn.Linear(emb_size, tgt_vocab_size)

        # Initialize parameters with Glorot / fan_avg.
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def get_src_mask(self, src):
        return torch.zeros((src.shape[0], src.shape[0]),device=device).type(torch.bool)

    def get_tgt_mask(self, tgt):
        return generate_subsequent_mask(tgt.shape[0])
    
    def encode(self, src, src_padding_mask):
        # Implement the encode function
        src_mask = self.get_src_mask(src)
        encode_emb = self.src_embed.forward(src)
        pos_encod = self.positional_encoding.forward(encode_emb)
        encode_out = self.encoder.forward(pos_encod, src_mask, src_padding_mask)
        return encode_out
    
    def decode(self, tgt, memory, src_padding_mask, tgt_padding_mask):
        # Implement the decode function
        tgt_mask = self.get_tgt_mask(tgt)
        decode_emb = self.tgt_embed.forward(tgt)
        pos_encod = self.positional_encoding.forward(decode_emb)
        decode_out = self.decoder.forward(pos_encod, memory, src_padding_mask, tgt_mask, tgt_padding_mask)
        return decode_out
    
    def forward(self, src, tgt, src_padding_mask, tgt_padding_mask):
        # Implement the forward pass
        memory = self.encode(src, src_padding_mask)
        decode_out = self.decode(tgt, memory, src_padding_mask, tgt_padding_mask)
        out = self.out(decode_out)
        return out

"""### Training the Model 

```
Epoch: 1, Train loss: 3.999, Val loss: 3.041, Epoch time = 46.511s
```
"""

seed_everything(SEED)
manual_transformer = ManualTransformer(num_encoder_layers=3, num_decoder_layers=3, emb_size=512, nhead=8, src_vocab_size=len(de_vocab), tgt_vocab_size=len(en_vocab))
loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)
optimizer = torch.optim.Adam(manual_transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)
train(manual_transformer, loss_fn, optimizer, n_epochs=15)

"""# Part 2: Evaluating our Transformer

BLEU is a metric between zero and one that measures the quality of a machine translation against a set of high-quality "ground truth" reference translations.

## Getting Translations Out of Our Model

### Greedy decoding to get outputs;
Gets predictions from the trained model
- `tokenlist_to_strlist(tokens, vocab)`, which takes in a list of tokens and outputs a list of the strings that correspond to each token
- `tokenlist_to_str(tokens, vocab, show_pad=False)`, which calls the function above and then turns the resulting list of strings into a single sentence. If `show_pad` is True, prints out the \<pad\> tokens.
- `translate_tokens(model, src_tokens)`, which takes in a Transformer model and tokens for a German source sentence and outputs a list of tokens that are an English translation of the sentence 
- `translate(model, src_sentence)`, which takes in a Transformer model and a German source sentence as a string, and returns a string English translation.
"""

# helper function to convert list of tokens to list of strings
def tokenlist_to_strlist(tokens, vocab):
  return vocab.lookup_tokens(list(tokens.cpu().numpy()))

# helper function to convert list of tokens to a string
def tokenlist_to_str(tokens, vocab, show_pad=False):
  out = " ".join(tokenlist_to_strlist(tokens, vocab)).replace("<bos>", "").replace("<eos>", "")
  if show_pad:
    return out
  else:
    return out.replace("<pad>", "")

# function to generate translation using greedy algorithm
def greedy_decode(model, src, max_len, start_symbol):
    src = src.to(device)
    src_padding_mask = padding_mask(src)

    memory = model.encode(src, src_padding_mask).to(device)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)
    for i in range(max_len-1):
        tgt_padding_mask = padding_mask(ys)
        out = model.decode(ys, memory, src_padding_mask, tgt_padding_mask).to(device) # shape (i+1, 1, 512)
        out = out.transpose(0, 1).to(device)
        prob = model.out(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.item()

        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        if next_word == EOS_IDX:
            break
    return ys.flatten()

# function that translates from a list of tokens to another list of tokens
def translate_tokens(model, src_tokens):
    model.eval()
    src_tokens = torch.cat([torch.tensor([BOS_IDX]), src_tokens, torch.tensor([EOS_IDX])], dim=0).view(-1, 1)
    num_tokens = src_tokens.shape[0]
    tgt_tokens = greedy_decode(model, src_tokens, max_len=num_tokens + 5, start_symbol=BOS_IDX)
    return tgt_tokens

# function that translates from a string sentence to a string version of the translation
def translate(model, src_sentence, output_tokens=False):
    model.eval()
    src_tokens = torch.tensor(de_vocab(de_tokenizer(src_sentence)), dtype=torch.long)  
    tgt_tokens = translate_tokens(model, src_tokens)
    translation = tokenlist_to_str(tgt_tokens, en_vocab)
    if output_tokens:
      return translation, src_tokens, tgt_tokens
    else:
      return translation

print(translate(manual_transformer, "Ein Mann auf einem Maisfeld mit einem Hund"))
print(translate(manual_transformer, "Drei Kinder essen Eis, w√§hrend ein Elternteil zuschaut"))
print(translate(manual_transformer, "Eine Frau am Strand fotografiert das Meer"))
print(translate(manual_transformer, "Ein Traktor bewegt Erde f√ºr den Bau einer St√ºtzmauer."))

"""## Evaluating Translations using BLEU

The overall formula for BLEU looks like:
$ \text{BLEU} = \min(1, \text{exp}(1 - \frac{\text{reference length}}{\text{output length}}))(\prod\limits_{i=1}^{4} precision_{i})^{1/4}$
"""

from typing import Coroutine
from nltk.translate.bleu_score import brevity_penalty
import math
import collections

# list_reference is a list of sentences and list_candidate is a list of sentences
# where each sentence is a list of tokens (integers or strings). 
def my_corpus_bleu(list_reference, list_candidate, n=4):
      #DONE: Implement the BLEU algorithm
      count = Counter()

      # find total length of list ref, list candidates
      len_ref = 0
      len_cand = 0
      for sentence in list_reference:
        len_ref += len(sentence) 
      for sentence in list_candidate:
        len_cand += len(sentence) 

      # calculate brevity penalty
      brevity_penalty = min(1, np.exp(1-(len_ref/len_cand)))
      precision_prod = 1 

      # find and iterate through all n-grams
      for i in range(1, n+1):
        count_candidate_g = 0
        i_precision = 0

        for j in range(len(list_candidate)): 
          ref_grams = Counter()
          cand_grams = Counter()

          # iterate through candidate sentences
          for q in range(len(list_candidate[j]) - i + 1):
            string_list = [str(x) for x in list_candidate[j][q:q+i]]
            g = " ".join(string_list)
            cand_grams[g] += 1
            ref_grams [g] = 0
            count_candidate_g += 1

          # find corresponding gram in reference sentences
          for r in range(len(list_reference[j]) - i + 1):
            str_list2 = [str(x) for x in list_reference[j][r:r+i]]
            g = " ".join(str_list2)
            if g in cand_grams.keys():
              ref_grams[g] += 1

          # calculate min
          for g in cand_grams.keys():
            i_precision += min(cand_grams[g], ref_grams[g])

        precision_i = i_precision/count_candidate_g
        precision_prod *= (i_precision/count_candidate_g)
      
      n_gram_overlap = precision_prod**(1/4)
      score = brevity_penalty*n_gram_overlap
      return score

"""### Unit testing against NLTK implementation

"""

from nltk.translate.bleu_score import corpus_bleu

toy_references = ["The NASA Opportunity rover is battling a massive dust storm on Mars .".split()]
toy_hypothesis = ["A NASA rover is fighting a massive storm on Mars .".split()]

toy_references2 = ["The quick brown fox jumped over the lazy dog".split(), "holy crap today is fring friday".split(), "jesse we have to cook". split()]
toy_hypothesis2 = ["The fast maroon fox jumped over the tired old dog".split(), "oh wow today is fring friday".split(), "jesse we need to cook".split()]

def compare_bleu(toy_references, toy_hypothesis):
  print("NLTK:", corpus_bleu([[lst] for lst in toy_references], toy_hypothesis))
  print("Mine:", my_corpus_bleu(toy_references, toy_hypothesis, 4))

compare_bleu(toy_references, toy_hypothesis)
print()
compare_bleu(toy_references2, toy_hypothesis2)

def get_nltk_bleu(model, val_data, en_vocab):
  predictions = [translate_tokens(model, t[0]).tolist()[1:-1] for t in val_data]
  labels = [[t[1].tolist()] for t in val_data] # labels is a list of lists of words 
  return corpus_bleu(labels, predictions)

def get_my_bleu(model, val_data, en_vocab):
  predictions = [translate_tokens(model, t[0]).tolist()[1:-1] for t in val_data]
  labels = [t[1].tolist() for t in val_data] # labels is a list of words
  return my_corpus_bleu(labels, predictions)

#print(get_nltk_bleu(manual_transformer, val_data, en_vocab))
print(get_my_bleu(manual_transformer, val_data, en_vocab))

"""# Analysis

Visualizes the cross-attention weights in the `ManualDecoderLayer` forward pass. Inputs a German sentence as a string to get a heatmap of the last layer of the decoder in the cross-attention sublayer.
"""

import matplotlib.pyplot as plt
def plot_attention(src_tokens, tgt_tokens, attn_weights):
    fig = plt.figure()
    attn_weights = attn_weights.cpu()
    cax = plt.matshow(attn_weights.detach().numpy(), cmap='bone')

    src_labels = ['<bos>'] + de_vocab.lookup_tokens(list(src_tokens.cpu().numpy())) + ['<eos>']
    tgt_labels = en_vocab.lookup_tokens(list(tgt_tokens.cpu().numpy()))[1:]

    plt.xticks(range(len(src_labels)), src_labels, rotation=80)
    plt.yticks(range(len(tgt_labels)), tgt_labels)

    plt.show()

def analyze_sentence(original):
  translation, src_tokens, tgt_tokens = translate(manual_transformer, original, output_tokens=True)
  attn_weights = manual_transformer.decoder.layers[-1].attn_weights.squeeze()

  plot_attention(src_tokens, tgt_tokens, attn_weights)

analyze_sentence("Drei Kinder essen Eis, w√§hrend ein Elternteil zuschaut")